module;
#include <algorithm>
#include <map>
#include <random>
#include <string>
#include <thread>
#include <vector>

export module gpt.common;

import ggml;

export
{
    // Remove it later
    int64_t ggml_time_us() { return 0; }

    constexpr int32_t GGML_QNT_VERSION_FACTOR = 1000; // do not change this
    constexpr uint32_t GGML_FILE_MAGIC = 0x67676d6c; // "ggml"

    struct gpt_params {
        int32_t seed = -1;   // RNG seed
        int32_t n_threads = std::min(4, (int32_t)std::thread::hardware_concurrency());
        int32_t n_predict = 200;  // new tokens to predict
        int32_t n_parallel = 1;    // number of parallel streams
        int32_t n_batch = 32;   // batch size for prompt processing
        int32_t n_ctx = 2048; // context size (this is the KV cache max size)
        int32_t n_gpu_layers = 0;    // number of layers to offlload to the GPU

        bool ignore_eos = false; // ignore EOS token when generating text

        // sampling parameters
        int32_t top_k = 40;
        float   top_p = 0.9f;
        float   temp = 0.9f;
        int32_t repeat_last_n = 64;
        float   repeat_penalty = 1.00f;

        std::string model = "models/gpt-2-117M/ggml-model.bin"; // model path
        std::string prompt = "";
        std::string token_test = "";

        bool    interactive = false;
        int32_t interactive_port = -1;
    };

    struct gpt_vocab {
        using id = int32_t;
        using token = std::string;

        std::map<token, id> token_to_id;
        std::map<id, token> id_to_token;
        std::vector<std::string> special_tokens;

        //void add_special_token(const std::string& token);
    };

    bool ggml_common_quantize_0(
        std::ifstream& finp,
        std::ofstream& fout,
        const ggml_ftype ftype,
        const std::vector<std::string>& to_quant,
        const std::vector<std::string>& to_skip);

    enum ggml_ftype ggml_parse_ftype(const char* str);
    void ggml_print_ftypes(FILE* fp = stderr);

    // test outputs of gpt_tokenize
    //
    //   - compare with tokens generated by the huggingface tokenizer
    //   - test cases are chosen based on the model's main language (under 'prompt' directory)
    //   - if all sentences are tokenized identically, print 'All tests passed.'
    //   - otherwise, print sentence, huggingface tokens, ggml tokens
    //
    void test_gpt_tokenizer(gpt_vocab& vocab, const std::string& fpath_test);

    // split text into tokens
    //
    // ref: https://github.com/openai/gpt-2/blob/a74da5d99abaaba920de8131d64da2862a8f213b/src/encoder.py#L53
    //
    // Regex (Python):
    // r"""'s|'t|'re|'ve|'m|'ll|'d| ?\p{L}+| ?\p{N}+| ?[^\s\p{L}\p{N}]+|\s+(?!\S)|\s+"""
    //
    // Regex (C++):
    // R"('s|'t|'re|'ve|'m|'ll|'d| ?[[:alpha:]]+| ?[[:digit:]]+| ?[^\s[:alpha:][:digit:]]+|\s+(?!\S)|\s+)"
    //
    std::vector<gpt_vocab::id> gpt_tokenize(const gpt_vocab& vocab, const std::string& text);

    // sample next token given probabilities for each embedding
    //
    //   - consider only the top K tokens
    //   - from them, consider only the top tokens with cumulative probability > P
    //
    // TODO: not sure if this implementation is correct
    // TODO: temperature is not implemented
    //
    gpt_vocab::id gpt_sample_top_k_top_p(
        const gpt_vocab& vocab,
        const float* logits,
        int    top_k,
        double top_p,
        double temp,
        std::mt19937& rng);

    bool gpt_params_parse(int argc, char** argv, gpt_params& params);
    std::string gpt_random_prompt(std::mt19937& rng);
}